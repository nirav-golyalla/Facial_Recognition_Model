{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import interp\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "import itertools\n",
    "\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "IMG_SIZE = 48\n",
    "\n",
    "atm=r'Chokepoint\\Youtube Faces\\Video\\frame_images_DB'\n",
    "face_cascade = cv2.CascadeClassifier('xml_file/haarcascade_frontalface_default.xml')\n",
    "directory_list = list()\n",
    "for root, dirs, files in os.walk(atm, topdown=False):\n",
    "    for name in dirs:\n",
    "        directory_list.append(os.path.join(root, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('xml_file/haarcascade_frontalface_default.xml')\n",
    "\n",
    "jpg_dir = 'Chokepoint\\lfw_jpg'\n",
    "\n",
    "training_data = []\n",
    "for img in tqdm(os.listdir(jpg_dir)):\n",
    "    path = os.path.join(jpg_dir,img)\n",
    "    gray = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(gray,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_color = gray[y:y+h, x:x+w]\n",
    "    \n",
    "    image = cv2.resize(roi_color, (IMG_SIZE,IMG_SIZE))\n",
    "    image = image.flatten()\n",
    "    training_data.append(np.array(image))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "training_data_vid = []\n",
    "for u in range(50):\n",
    "    for img in tqdm(os.listdir(directory_list[u])):\n",
    "        \n",
    "        path = os.path.join(directory_list[u],img)\n",
    "        gray = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for(x,y,w,h) in faces:\n",
    "            cv2.rectangle(gray,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            roi_color = gray[y:y+h, x:x+w]\n",
    "\n",
    "        image = cv2.resize(roi_color, (IMG_SIZE,IMG_SIZE))\n",
    "  \n",
    "        dirname = os.path.split(os.path.split(directory_list[u])[0])[1]\n",
    "\n",
    "        training_data_vid.append([np.array(image),dirname])  \n",
    "        labels.append(dirname)\n",
    "\n",
    "values = array(labels)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)\n",
    "\n",
    "for i in range(len(training_data_vid)):\n",
    "    training_data_vid[i][1] = np.array(onehot_encoded[i])\n",
    "\n",
    "    \n",
    "print(training_data_vid[1])\n",
    "print(training_data_vid[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and Splitting of Dataset\n",
    "#train_data = process_images_training()\n",
    "\n",
    "\n",
    "train = training_data[:-2000]\n",
    "test = training_data[-2000:]\n",
    "\n",
    "train_vid = training_data_vid[:-99]\n",
    "test_vid = training_data_vid[-99:]\n",
    "train_integer_encoded = integer_encoded[:-99]\n",
    "test_integer_encoded = integer_encoded[-99:]\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(train_vid))\n",
    "print(len(test_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions of layers\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = initializer(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape=shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def deconv2d(x, W,out):\n",
    "    return tf.nn.conv2d_transpose(x,W,strides=[1,2,2,1],output_shape = out,padding='SAME' )\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape, bias):\n",
    "    W = shape\n",
    "    b = bias_variable(bias)\n",
    "    y = tf.nn.dropout(tf.nn.softplus(conv2d(input, W)+b), dropout)\n",
    "    return y\n",
    "def deconv_layer(input, shape, bias,out):\n",
    "    W = shape\n",
    "    b = bias_variable(bias)\n",
    "    return tf.nn.dropout(tf.nn.softplus(deconv2d(input, W,tf.stack(out))+b), dropout)\n",
    "\n",
    "def full_layer(input, shape, bias):\n",
    "    W = shape\n",
    "    b = bias_variable(bias)\n",
    "    return tf.nn.softplus(tf.add(tf.matmul(input, W),b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_epochs = 2\n",
    "LearningRate = math.pow(10, -4) * 1\n",
    "batch_size = 101\n",
    "dropout=0.95\n",
    "peopleNum = 30\n",
    "corruption_level = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensions MyData\n",
    "n1=(int)(2304)\n",
    "n2=(int)(n1/2)\n",
    "n3=(int)(n2/2)\n",
    "n4=(int)(n3/2)\n",
    "n5=(int)(n4)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'ce0': weight_variable([2,2,1,n1]),\n",
    "    'ce1': weight_variable([2,2,n1,n2]),\n",
    "    'ce2': weight_variable([2,2,n2,n3]),\n",
    "    'ce3': weight_variable([2,2,n3,n4]),\n",
    "    'ce4': weight_variable([2,2,n4,n5]),\n",
    "    'cd4': weight_variable([2,2,n4,n5]),\n",
    "    'cd3': weight_variable([2,2,n3,n4]),\n",
    "    'cd2': weight_variable([2,2,n2,n3]),\n",
    "    'cd1': weight_variable([2,2,n1,n2]),\n",
    "    'cd0': weight_variable([2,2,1,n1]),  \n",
    "}\n",
    "\n",
    "weights2= {\n",
    "    'd0': weight_variable([2*2*288, 2048]),\n",
    "    'd1': weight_variable([2048, peopleNum]), \n",
    "}\n",
    "\n",
    "print(weights['ce0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    conv0 = conv_layer(x, weights['ce0'], bias=[n1])\n",
    "    conv1 = conv_layer(conv0, weights['ce1'], bias=[n2])\n",
    "    conv2 = conv_layer(conv1, weights['ce2'], bias=[n3])\n",
    "    conv3 = conv_layer(conv2, weights['ce3'], bias=[n4])\n",
    "    conv4 = conv_layer(conv3, weights['ce4'], bias=[n5])\n",
    "    \n",
    "    return conv4\n",
    "\n",
    "def autoencoder(x):\n",
    "    #encoder\n",
    "    data = tf.reshape(x, shape = [-1,IMG_SIZE,IMG_SIZE,1])\n",
    "    conv4 = encoder(data)\n",
    "    \n",
    "    #decoder\n",
    "    deconv4 = deconv_layer(conv4, weights['cd4'], bias=[n4],out=[tf.shape(x)[0], 3, 3, n4])\n",
    "    deconv3 = deconv_layer(deconv4, weights['cd3'], bias=[n3],out=[tf.shape(x)[0], 6,6, n3])\n",
    "    deconv2 = deconv_layer(deconv3, weights['cd2'], bias=[n2],out=[tf.shape(x)[0], 12, 12, n2])\n",
    "    deconv1 = deconv_layer(deconv2, weights['cd1'], bias=[n1],out=[tf.shape(x)[0], 24, 24, n1])\n",
    "    deconv0 = deconv_layer(deconv1, weights['cd0'], bias=[1],out=[tf.shape(x)[0], 48, 48, 1])\n",
    "    return deconv0\n",
    "\n",
    "def autoCNN(x):\n",
    "    data = tf.reshape(x, shape = [-1,48,48,1])\n",
    "    conv4 = encoder(data)\n",
    "    \n",
    "    conv6_flat = tf.reshape(conv4, [-1,2*2*288])\n",
    " \n",
    "    fc_1= full_layer(conv6_flat, weights2['d0'], [2048])\n",
    "    fc_1 = tf.nn.dropout(fc_1, dropout)\n",
    "    output = tf.matmul(fc_1, weights2['d1']) + bias_variable([peopleNum])\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training Phase\n",
    "def training(x,x_):   \n",
    "    \n",
    "    prediction1 = autoencoder(x)\n",
    "    print(prediction1.shape)\n",
    "\n",
    "    loss1 = tf.reduce_mean(tf.square(prediction1 - tf.reshape(x, shape=[-1, 48,48,1])))\n",
    "    optimizer = tf.train.AdamOptimizer(LearningRate)\n",
    "    optimizer1 = optimizer.minimize(loss1,var_list=[weights])\n",
    "    loss_list = []\n",
    "    \n",
    "    prediction2 = autoCNN(x_)\n",
    "      \n",
    "    cost = tf.reduce_mean( tf.losses.softmax_cross_entropy(logits=prediction2, onehot_labels= y_) )\n",
    "    optimizer2 = optimizer.minimize(cost,var_list=[weights2])\n",
    "    correct = tf.equal(tf.argmax(prediction2, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))       \n",
    "    loss_list2 = []\n",
    "    training_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        mean_img = np.zeros(2304)\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "          \n",
    "            while i < len(train):\n",
    "                start = i \n",
    "                end = i + batch_size\n",
    "             \n",
    "                batch_x = np.array(train[start:end])\n",
    "               \n",
    "                _, c= sess.run([optimizer1,loss1], feed_dict={x: batch_x})\n",
    "                epoch_loss += c\n",
    "                i+= batch_size\n",
    "               \n",
    "            loss_list.append(epoch_loss)\n",
    "            if (epoch+1)%1 == 0:\n",
    "                print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "        \n",
    "            #Plotting Results every 5 epochs\n",
    "                n_examples = 5\n",
    "                test_xs = np.array(test[:n_examples])\n",
    "                recon = sess.run(prediction1, feed_dict={x: test_xs})\n",
    "                fig, axs = plt.subplots(2, n_examples, figsize=(15, 4))\n",
    "                for example_i in range(n_examples):\n",
    "                    axs[0][example_i].matshow(np.reshape(test_xs[example_i, :], (48, 48)), cmap=plt.get_cmap('gray'))\n",
    "                    axs[1][example_i].matshow(np.reshape(np.reshape(recon[example_i, ...], (2304,)) + mean_img, (48, 48)), cmap=plt.get_cmap('gray'))\n",
    "                    plt.show()\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_vid):\n",
    "                start = i \n",
    "                end = i + batch_size\n",
    "\n",
    "                batch_x = np.array(X_data[start:end])\n",
    "                batch_y = np.array(Y_labels[start:end])\n",
    "\n",
    "                _, c = sess.run([optimizer2,cost], feed_dict={x_: batch_x, y_: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+= batch_size\n",
    "                print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "                loss_list2.append(epoch_loss)\n",
    "                if (epoch+1)%1 == 0:\n",
    "                    training_acc_list.append(training_acc_CNN(sess, accuracy))\n",
    "                    test_acc_list.append(validation_acc_CNN(sess, accuracy,33))\n",
    "\n",
    "    predictions = prediction2.eval(feed_dict={x_: cX_data}, session=sess)\n",
    "    plt.plot(loss_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.show()\n",
    "    plt.plot(training_acc_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.show()\n",
    "    plt.plot(test_acc_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Testing Accuracy\")\n",
    "    plt.show()\n",
    "    roc_auc_graph(\"Testing\",predictions,cY_labels) \n",
    "    confusion_matrix_make(sess,predictions,cY_labels)\n",
    "\n",
    "    precision_recall(sess,predictions,cY_labels)\n",
    "    perf_measure(cY_labels, predictions)\n",
    "\n",
    "    live_prediction(sess,prediction2)\n",
    "\n",
    "    print(\"Done\")  \n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,2304])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = tf.placeholder(tf.float32, shape=[None,48,48,1], name='placeholder_input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, peopleNum], name='placeholder_labels') \n",
    "\n",
    "y_true_cls = tf.argmax(y_, axis=1)\n",
    "\n",
    "X_data = np.array([i[0] for i in train_vid]).reshape(-1,48,48,1)\n",
    "Y_labels = [i[1] for i in train_vid]#.reshape(-1,peopleNum)\n",
    "\n",
    "print(len(Y_labels))\n",
    "\n",
    "cX_data = np.array([i[0] for i in test_vid]).reshape(-1,48,48,1)\n",
    "cY_labels = [i[1] for i in test_vid]#.reshape(-1,peopleNum)\n",
    "\n",
    "print(len(cX_data))\n",
    "print(y_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training(x,x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_acc_CNN(sess, accuracy):     \n",
    "    training_accuracy_results = []\n",
    "    cls_pred = np.zeros(shape=len(test_vid), dtype=np.int)\n",
    "    i = 0\n",
    "    while i < len(train_vid):\n",
    "        start = i \n",
    "        end = i + batch_size\n",
    "\n",
    "        batch_x = np.array(X_data[start:end])\n",
    "        batch_y = np.array(Y_labels[start:end])\n",
    "\n",
    "        training_accuracy_results.append(sess.run(accuracy, feed_dict={x_: batch_x, y_: batch_y})) \n",
    "        i+= batch_size\n",
    "        \n",
    "    acc = np.mean(training_accuracy_results)\n",
    "    print(\"Training Accuracy: {}\".format(acc *100))\n",
    "    return acc\n",
    "def validation_acc_CNN(sess, accuracy,batch_size):     \n",
    "    validation_accuracy_results = []\n",
    "    cls_pred = np.zeros(shape=len(test_vid), dtype=np.int)\n",
    "    i = 0\n",
    "    while i < len(test_vid):\n",
    "        start = i \n",
    "        end = i + batch_size\n",
    "\n",
    "        batch_x = np.array(cX_data[start:end])\n",
    "        batch_y = np.array(cY_labels[start:end])\n",
    "\n",
    "        validation_accuracy_results.append(sess.run(accuracy, feed_dict={x_: batch_x, y_: batch_y})) \n",
    "        i+= batch_size\n",
    "        \n",
    "    acc = np.mean(validation_accuracy_results)\n",
    "    print(\"Validation Accuracy: {}\".format(acc*100))\n",
    "    return acc\n",
    "    \n",
    "def roc_auc_graph(typ,predictions,labels):\n",
    "    test = label_binarize(test_integer_encoded, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "    fpr, tpr, _ = roc_curve(test.ravel(), predictions.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    print('roc_auc: {}'.format(roc_auc))\n",
    "    figure(num=None, figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('{} Receiver operating characteristic'.format(typ))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm,names, title='Confusion matrix', cmap=plt.cm.Blues,normalize=False):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=90)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def confusion_matrix_make(typ,prediction,labels):\n",
    "    test = label_binarize(test_integer_encoded, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "    y_test2 = np.argmax(test,axis=1)\n",
    "    pred = np.argmax(prediction,axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test2, pred, labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print('Normalized confusion matrix')\n",
    "\n",
    "    figure(num=None, figsize=(4,4), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plot_confusion_matrix(cm_normalized, people, title='{} Normalized confusion matrix'.format(typ), normalize = True)\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "def precision_recall(typ,predictions,labels):\n",
    "    test = label_binarize(test_integer_encoded, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "    precision, recall, _ = precision_recall_curve(test.ravel(), predictions.ravel())\n",
    "    \n",
    "    print('precision: {}'.format(precision))\n",
    "    print('recall: {}'.format(recall))\n",
    "    \n",
    "    \n",
    "    average_precision = average_precision_score(test.ravel(), predictions.ravel())\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision))\n",
    "    \n",
    "    figure(num=None, figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('{typ} Average precision score, micro-averaged over all classes: AP={average_precision}'\n",
    "        .format(average_precision=average_precision,typ=typ))\n",
    "    \n",
    "def perf_measure(labels, prediction):\n",
    "    test = label_binarize(test_integer_encoded, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "    y_test2 = np.argmax(test,axis=1)\n",
    "    pred = np.argmax(prediction,axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test2, pred, labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    print('tp: {}'.format(TP))\n",
    "    print('fp: {}'.format(FP))\n",
    "    print('tN: {}'.format(TN))\n",
    "    print('fn: {}'.format(FN)) \n",
    "    \n",
    "    \n",
    "def live_prediction(sess,prediction):\n",
    "    fig=plt.figure()\n",
    "\n",
    "    for num,data in enumerate(test[:12]):\n",
    "        img_num = data[1]\n",
    "        img_data = data[0]\n",
    "\n",
    "        y = fig.add_subplot(3,4,num+1)\n",
    "        orig = img_data\n",
    "        data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n",
    "        \n",
    "        model_out = prediction.eval(feed_dict={x: [data]}, session=sess)\n",
    "        if np.argmax(model_out) == 0: str_label='person1'\n",
    "        elif np.argmax(model_out) == 1: str_label='person3'\n",
    "        elif np.argmax(model_out) == 2: str_label='person4'\n",
    "        elif np.argmax(model_out) == 3: str_label='person5'\n",
    "        elif np.argmax(model_out) == 4: str_label='person6'\n",
    "        elif np.argmax(model_out) == 5: str_label='person7'\n",
    "        elif np.argmax(model_out) == 6: str_label='person9'\n",
    "        elif np.argmax(model_out) == 7: str_label='person10'\n",
    "        elif np.argmax(model_out) == 8: str_label='person11'\n",
    "        elif np.argmax(model_out) == 9: str_label='person12'\n",
    "        elif np.argmax(model_out) == 10: str_label='person13'\n",
    "        elif np.argmax(model_out) == 11: str_label='person14'\n",
    "        elif np.argmax(model_out) == 12: str_label='person15'\n",
    "        elif np.argmax(model_out) == 13: str_label='person16'\n",
    "        elif np.argmax(model_out) == 14: str_label='person17'\n",
    "        elif np.argmax(model_out) == 15: str_label='person18'\n",
    "        elif np.argmax(model_out) == 16: str_label='person19'\n",
    "        elif np.argmax(model_out) == 17: str_label='person20'\n",
    "        elif np.argmax(model_out) == 18: str_label='person21'\n",
    "        elif np.argmax(model_out) == 19: str_label='person22'\n",
    "        else: str_label='unknown'\n",
    "\n",
    "        y.imshow(orig,cmap='gray')\n",
    "        plt.title(str_label)\n",
    "        y.axes.get_xaxis().set_visible(False)\n",
    "        y.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "            \n",
    "        \n",
    "\n",
    "def image_labeler_converter(labelName):\n",
    "    if np.array_equal(labelName,[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 0\n",
    "    elif np.array_equal(labelName,[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 1\n",
    "    elif np.array_equal(labelName,[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 2\n",
    "    elif np.array_equal(labelName,[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 3\n",
    "    elif np.array_equal(labelName,[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 4\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 5\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 6\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]): return 7\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]): return 8\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]): return 9\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]): return 10\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0]): return 11\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]): return 12\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]): return 13\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]): return 14\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]): return 15\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]): return 16\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]): return 17\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]): return 18\n",
    "    elif np.array_equal(labelName,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]): return 19\n",
    "    else: return 20        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_CNN(data):   \n",
    "    prediction = autoCNN(data)\n",
    "    print(prediction.shape)\n",
    "    print(y_.shape)\n",
    "    tf.identity(prediction, name=\"Output_ph\")\n",
    "    cost = tf.reduce_mean( tf.losses.softmax_cross_entropy(logits=prediction, onehot_labels= y_) )\n",
    "    optimizer = tf.train.AdamOptimizer(LearningRate).minimize(cost)\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))       \n",
    "    loss_list = []\n",
    "    training_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_vid):\n",
    "                start = i \n",
    "                end = i + batch_size\n",
    "\n",
    "                batch_x = np.array(X_data[start:end])\n",
    "                batch_y = np.array(Y_labels[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer,cost], feed_dict={x_: batch_x, y_: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+= batch_size\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            loss_list.append(epoch_loss)\n",
    "            if (epoch+1)%1 == 0:\n",
    "                training_acc_list.append(training_acc_CNN(sess, accuracy))\n",
    "                test_acc_list.append(validation_acc_CNN(sess, accuracy,33))\n",
    "\n",
    "        predictions = prediction.eval(feed_dict={x_: cX_data}, session=sess)\n",
    "        \n",
    "        print(prediction)\n",
    "        print(predictions)\n",
    "        \n",
    "        plt.plot(loss_list)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.show()\n",
    "        plt.plot(training_acc_list)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.show()\n",
    "        plt.plot(test_acc_list)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(\"Testing Accuracy\")\n",
    "        plt.show()\n",
    "        roc_auc_graph(\"Testing\",predictions,cY_labels) \n",
    "        confusion_matrix_make(sess,predictions,cY_labels)\n",
    "\n",
    "        precision_recall(sess,predictions,cY_labels)\n",
    "        perf_measure(cY_labels, predictions)\n",
    "        \n",
    "        live_prediction(sess,prediction)\n",
    "        \n",
    "        print('\\nSaving...')\n",
    "        inputs_dict = {\n",
    "            \"features_data_ph\": x,\n",
    "            \"labels_data_ph\": y_\n",
    "        }\n",
    "        outputs_dict = {\n",
    "            \"Output_ph\": prediction\n",
    "        }\n",
    "        #tf.saved_model.simple_save(sess, file_path, inputs_dict, outputs_dict)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_CNN(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
